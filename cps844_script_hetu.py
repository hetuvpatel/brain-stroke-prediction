# -*- coding: utf-8 -*-
"""cps844_script_hetu.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jgS5OhntiZSgC0nOAxXHaTfI-8YK6H8t
"""

# üì¶ Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import (
    accuracy_score, f1_score, classification_report,
    roc_auc_score, confusion_matrix, roc_curve, ConfusionMatrixDisplay
)
from sklearn.impute import SimpleImputer

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, StackingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier

from sklearn.feature_selection import RFE
from imblearn.over_sampling import SMOTE

import warnings
warnings.filterwarnings('ignore')

# üìÅ Load dataset
df = pd.read_csv('/content/healthcare-dataset-stroke-data.csv')

# üßπ Preprocessing
df.drop(columns=['id'], inplace=True)
df['bmi'] = pd.to_numeric(df['bmi'], errors='coerce')
df['bmi'] = SimpleImputer(strategy='mean').fit_transform(df[['bmi']])
label_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']
df[label_cols] = df[label_cols].apply(LabelEncoder().fit_transform)

# üéØ Define features and target
X = df.drop(columns='stroke')
y = df['stroke']

# üîç EDA Visualizations
plt.figure(figsize=(14, 6))
plt.subplot(1, 2, 1)
sns.countplot(data=df, x='stroke')
plt.title('Stroke Class Distribution')
plt.subplot(1, 2, 2)
df['stroke'].value_counts().plot.pie(autopct='%1.1f%%', labels=['No Stroke', 'Stroke'], colors=['skyblue', 'salmon'])
plt.title('Stroke Class Ratio')
plt.ylabel('')
plt.show()

# üí° Feature Correlation Heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.title('Feature Correlation Heatmap')
plt.show()

# üîÑ Scaling and Balancing
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_df = pd.DataFrame(X_scaled, columns=X.columns)

# Resample using SMOTE
X_resampled, y_resampled = SMOTE(random_state=42).fit_resample(X_df, y)
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

# üìä Model Definitions
models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "KNN": KNeighborsClassifier(),
    "SVM": SVC(probability=True)
}

# üß† Evaluation Function
def evaluate_model(name, model, X_test, y_test, return_results=False):
    preds = model.predict(X_test)
    probs = model.predict_proba(X_test)[:, 1]
    acc = accuracy_score(y_test, preds)
    f1 = f1_score(y_test, preds)
    auc = roc_auc_score(y_test, probs)

    print(f"\n--- {name} ---")
    print("Accuracy:", acc)
    print("F1 Score:", f1)
    print("ROC AUC:", auc)
    print("Confusion Matrix:\n", confusion_matrix(y_test, preds))
    print("Classification Report:\n", classification_report(y_test, preds))
    if return_results:
        return acc, f1, auc

# üß† Train & Evaluate on Full Features
metrics = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    acc, f1, auc = evaluate_model(name, model, X_test, y_test, return_results=True)
    metrics[name] = {'Accuracy': acc, 'F1 Score': f1, 'ROC AUC': auc}

# üìà Visual: Model Comparison
metric_df = pd.DataFrame(metrics).T
metric_df.plot(kind='bar', figsize=(10, 6), colormap='tab20')
plt.title('Model Comparison on Full Feature Set')
plt.ylabel('Score')
plt.xticks(rotation=45)
plt.grid(axis='y')
plt.legend(loc='lower right')
plt.tight_layout()
plt.show()

# üîß Feature Importance (Random Forest)
rf = models['Random Forest']
importances = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)
importances.plot(kind='barh', figsize=(10, 6), title='Random Forest Feature Importance')
plt.gca().invert_yaxis()
plt.show()

# üîß Feature Importance (Logistic Regression)
lr = models['Logistic Regression']
coefficients = pd.Series(lr.coef_[0], index=X.columns)
coefficients.sort_values().plot(kind='barh', title='Logistic Regression Coefficients', figsize=(10, 6))
plt.grid(True)
plt.show()

# üîç RFE Feature Selection
rfe_selector = RFE(LogisticRegression(), n_features_to_select=7)
X_rfe = rfe_selector.fit_transform(X_df, y)
selected_cols = X.columns[rfe_selector.support_]
print("RFE Selected Features:", selected_cols.tolist())

X_rfe_df = X_df[selected_cols]
X_rfe_res, y_rfe_res = SMOTE(random_state=42).fit_resample(X_rfe_df, y)
X_train_fs, X_test_fs, y_train_fs, y_test_fs = train_test_split(X_rfe_res, y_rfe_res, test_size=0.2, random_state=42)

# üß† Re-train on RFE-selected Features
for name, model in models.items():
    model.fit(X_train_fs, y_train_fs)
    evaluate_model(f"{name} (RFE)", model, X_test_fs, y_test_fs)

# üîß Tuned SVM
svm_grid = {'C': [1, 10], 'kernel': ['rbf'], 'gamma': ['scale']}
svm_tuned = GridSearchCV(SVC(probability=True), svm_grid, scoring='f1', cv=5)
svm_tuned.fit(X_train, y_train)
evaluate_model("Tuned SVM", svm_tuned.best_estimator_, X_test, y_test)

# üß† Stacking Ensemble
stacking_model = StackingClassifier(
    estimators=[
        ('lr', LogisticRegression()),
        ('rf', RandomForestClassifier()),
        ('knn', KNeighborsClassifier())
    ],
    final_estimator=SVC(probability=True)
)
stacking_model.fit(X_train, y_train)
evaluate_model("Stacked Ensemble", stacking_model, X_test, y_test)

# üìà Visual: ROC Curves
plt.figure(figsize=(12, 6))

# RFE models
plt.subplot(1, 2, 1)
for name, model in models.items():
    probs = model.predict_proba(X_test_fs)[:, 1]
    fpr, tpr, _ = roc_curve(y_test_fs, probs)
    plt.plot(fpr, tpr, label=f"{name} (RFE)")
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.title("ROC Curve (RFE Features)")
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.legend()
plt.grid(True)

# Full models
plt.subplot(1, 2, 2)
for name, model in [("Tuned SVM", svm_tuned.best_estimator_), ("Stacked Ensemble", stacking_model)]:
    probs = model.predict_proba(X_test)[:, 1]
    fpr, tpr, _ = roc_curve(y_test, probs)
    plt.plot(fpr, tpr, label=name)
plt.plot([0, 1], [0, 1], linestyle='--', color='gray')
plt.title("ROC Curve (Full Features)")
plt.xlabel("FPR")
plt.ylabel("TPR")
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

# üìâ Confusion Matrices
for name, model in models.items():
    model.fit(X_train, y_train)
    preds = model.predict(X_test)
    ConfusionMatrixDisplay.from_estimator(model, X_test, y_test)
    plt.title(f"Confusion Matrix: {name}")
    plt.show()

# üéØ Predictions Visualization (Stacked Ensemble)
y_pred_stacked = stacking_model.predict(X_test)
plt.figure(figsize=(8, 4))
sns.histplot(y_pred_stacked, bins=3, kde=False)
plt.title("Prediction Distribution - Stacked Ensemble")
plt.xlabel("Predicted Label")
plt.ylabel("Count")
plt.grid(True)
plt.show()